{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook is used to perofrm the supervised tweet classification experment for the Birdwatch metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report\n",
    "import pickle\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file 'hawkeye_metric_tweet_vectors_supervised.pickle' stores a vector for each tweet. This file is generated by the 'tweet_vectors_baseline.py' script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/hawkeye_metric_tweet_vectors_supervised.pickle', 'rb') as handle:\n",
    "    tweet_vectors = pickle.load(handle)\n",
    "\n",
    "notes = pd.read_csv(\"..//data//notes-00000-13-04-21.tsv\", sep='\\t')\n",
    "ratings = pd.read_csv(\"..//data//ratings-00000-13-04-21.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_annotation_df_next = pd.read_csv('tweet_groudtruth_annonation_next.csv',encoding='utf-8-sig')\n",
    "tweet_annotation_df = pd.read_csv('tweet_groudtruth_annonation.csv',encoding='utf-8-sig')\n",
    "tweet_annotation = pd.concat([tweet_annotation_df, tweet_annotation_df_next])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The true labels for tweets are obtained form the above annotation sheets. The  predicted  labels  for  the  tweets  for  both  the are determined as follows. For  the  Birdwatch  system,  we  count  the  number  of  helpful notes as the notes that have a helpfulness ratio of at least 0.84 (this  threshold  is  used  by  Birdwatch).  For  each  tweet,  if  the number of  helpful notes  that labeled  the tweet  as misleading are   more   than   or   equal   to   the   number   of   helpful   notes that  labeled  the  tweet  as  not-misleading,  we  say  Birdwatch classifies  the  tweet  as  misleading.  We  use  this  technique  as Birdwatch does not define a standard way for tweet labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.99      0.40       124\n",
      "           1       0.75      0.01      0.02       376\n",
      "\n",
      "    accuracy                           0.25       500\n",
      "   macro avg       0.50      0.50      0.21       500\n",
      "weighted avg       0.63      0.25      0.11       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true,y_pred = [],[]\n",
    "for idx,row in tweet_annotation.iterrows():\n",
    "    \n",
    "    y_true.append(row['human_annotation'])\n",
    "    notesForTweet = notes.loc[notes['tweetId'] == int(row['tweet_id'][:-2])]\n",
    "    \n",
    "    ratingsWithNotesForTweet = notesForTweet.set_index('noteId').join(ratings.set_index('noteId'), lsuffix=\"_note\", rsuffix=\"_rating\", how='left')\n",
    "    ratingsWithNotesForTweet['numRatings'] = ratingsWithNotesForTweet.apply(lambda x: 0 if math.isnan(x['helpful']) else 1, axis=1)    \n",
    "    \n",
    "    scoredNotes = ratingsWithNotesForTweet.groupby(['noteId']).agg({'helpful':'sum', 'numRatings' : 'sum', 'classification':'first'})\n",
    "    scoredNotes['helpfulnessRatio'] = scoredNotes['helpful']/scoredNotes['numRatings']\n",
    "    scoredNotes = scoredNotes[scoredNotes['helpfulnessRatio'] >= 0.84]\n",
    "\n",
    "    scoredNotes_misleading = scoredNotes.loc[scoredNotes['classification'] == 'MISINFORMED_OR_POTENTIALLY_MISLEADING']\n",
    "    scoredNotes_notmisleading = scoredNotes.loc[scoredNotes['classification'] == 'NOT_MISLEADING']\n",
    "    \n",
    "    if len(scoredNotes_misleading) >= len(scoredNotes_notmisleading): # >= scoredNotesNotMisleading['helpful'].sum():\n",
    "        y_pred.append(0)\n",
    "    else:\n",
    "        y_pred.append(1)\n",
    "\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
