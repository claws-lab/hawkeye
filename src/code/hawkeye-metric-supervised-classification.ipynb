{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pickle\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/tweet_vectors_supervised.pickle', 'rb') as handle:\n",
    "    tweet_vectors = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_annotation_df_next = pd.read_csv('tweet_groudtruth_annonation_next.csv',encoding='utf-8-sig')\n",
    "tweet_annotation_df = pd.read_csv('tweet_groudtruth_annonation.csv',encoding='utf-8-sig')\n",
    "tweet_annotation = pd.concat([tweet_annotation_df, tweet_annotation_df_next])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = [],[]\n",
    "annotated_tweets = list(set(tweet_annotation['tweet_id']))\n",
    "for idx,row in tweet_annotation.iterrows():\n",
    "    y.append(row['human_annotation'])\n",
    "    X.append(tweet_vectors[int(row['tweet_id'][:-2])])\n",
    "X,y = np.array(X),np.array(y)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) #Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   0.5s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.4s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................................. , total=   0.5s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   0.5s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   0.6s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   0.7s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   0.5s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   0.5s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   0.6s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   0.6s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   0.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    5.7s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate,cross_val_score\n",
    "clf = RandomForestClassifier(random_state=0)\n",
    "scoring = ['accuracy','precision','recall','precision_macro','recall_macro','f1_macro','precision_weighted','recall_weighted','f1_weighted','roc_auc']\n",
    "scores = cross_validate(clf, X, y, scoring=scoring, cv=10, verbose=2, n_jobs=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average fit_time = 0.5593947887420654\n",
      "Average score_time = 0.020790982246398925\n",
      "Average test_accuracy = 0.788\n",
      "Average test_precision = 0.8630190694658777\n",
      "Average test_recall = 0.8560455192034139\n",
      "Average test_precision_macro = 0.7165816628971723\n",
      "Average test_recall_macro = 0.7180868621658095\n",
      "Average test_f1_macro = 0.7137298208553615\n",
      "Average test_precision_weighted = 0.7902641581796916\n",
      "Average test_recall_weighted = 0.788\n",
      "Average test_f1_weighted = 0.7866815688228124\n",
      "Average test_roc_auc = 0.8624648940438414\n"
     ]
    }
   ],
   "source": [
    "# print(scores)\n",
    "for k,v in scores.items():\n",
    "    print(\"Average \" + k + \" = \" + str(v.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "n_estimators = [50, 60,70, 80, 90, 100]\n",
    "max_depth = [10,15,20]\n",
    "min_samples_split = [2, 5, 7, 10]\n",
    "min_samples_leaf = [1, 2, 5, 10]\n",
    "hyperF = dict(n_estimators = n_estimators, max_depth = max_depth,  \n",
    "              min_samples_split = min_samples_split, \n",
    "             min_samples_leaf = min_samples_leaf)\n",
    "rf_features = RandomForestClassifier()\n",
    "clf_rf_features = GridSearchCV(rf_features, hyperF, scoring = ['accuracy','precision','recall','f1_macro','roc_auc'],cv = 10, verbose = 3, n_jobs = -1)\n",
    "clf_rf_features.fit(X, y)\n",
    "\n",
    "best_params = clf_rf_features.best_params_\n",
    "val_score = clf_rf_features.best_score_\n",
    "print(best_params)\n",
    "print(\"VALIDATION SCORE =\", val_score)\n",
    "\n",
    "clf_predictions = clf_rf_features.predict(X_test)\n",
    "print(classification_report(y_test,clf_predictions))\n",
    "\n",
    "best_rf = RandomForestClassifier(max_depth=15, min_samples_leaf=1, min_samples_split=5, n_estimators=80)\n",
    "best_rf.fit(X_train, y_train)\n",
    "best_rf_predictions = best_rf.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "# Compute fpr, tpr, thresholds and roc auc\n",
    "fpr, tpr, thresholds = roc_curve(y_test, best_rf_predictions)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # random predictions curve\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate or (1 - Specifity)')\n",
    "plt.ylabel('True Positive Rate or (Sensitivity)')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['explained_variance', 'r2', 'max_error', 'neg_median_absolute_error', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_root_mean_squared_error', 'neg_mean_poisson_deviance', 'neg_mean_gamma_deviance', 'accuracy', 'roc_auc', 'roc_auc_ovr', 'roc_auc_ovo', 'roc_auc_ovr_weighted', 'roc_auc_ovo_weighted', 'balanced_accuracy', 'average_precision', 'neg_log_loss', 'neg_brier_score', 'adjusted_rand_score', 'homogeneity_score', 'completeness_score', 'v_measure_score', 'mutual_info_score', 'adjusted_mutual_info_score', 'normalized_mutual_info_score', 'fowlkes_mallows_score', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'jaccard', 'jaccard_macro', 'jaccard_micro', 'jaccard_samples', 'jaccard_weighted'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.SCORERS.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
