{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook is used to perform the supervised tweet classification experment for the Birdwatch metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pickle\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file \"hawkeye_metric_tweet_vectors_supervised.pickle\" stores a vector for each tweet. This file is generated by the \"hawkeye-metric-generate-supervised-vectors.py\" script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/hawkeye_metric_tweet_vectors_supervised.pickle', 'rb') as handle:\n",
    "    tweet_vectors = pickle.load(handle)\n",
    "\n",
    "tweet_annotation_df_next = pd.read_csv('tweet_groudtruth_annonation_next.csv',encoding='utf-8-sig')\n",
    "tweet_annotation_df = pd.read_csv('tweet_groudtruth_annonation.csv',encoding='utf-8-sig')\n",
    "tweet_annotation = pd.concat([tweet_annotation_df, tweet_annotation_df_next])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The true labels for tweets are obtained form the above annotation sheets. The  predicted  labels  for  the  tweets  for are determined as follows. For  the  Hawkeye  system,  sing  these  vectors,  we  train  arandom  forest  classifier  and  conduct  10-fold  cross  validationon  the  500  tweets.  Average  numbers  across  the  ten  folds  arecalculated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = [],[]\n",
    "annotated_tweets = list(set(tweet_annotation['tweet_id']))\n",
    "for idx,row in tweet_annotation.iterrows():\n",
    "    y.append(row['human_annotation'])\n",
    "    X.append(tweet_vectors[int(row['tweet_id'][:-2])])\n",
    "X,y = np.array(X),np.array(y)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) #Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   0.6s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................................. , total=   0.5s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   0.5s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   0.5s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   0.5s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   0.5s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   0.6s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   0.5s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   0.5s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    5.0s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate,cross_val_score\n",
    "clf = RandomForestClassifier(random_state=0)\n",
    "scoring = ['accuracy','precision','recall','precision_macro','recall_macro','f1_macro','precision_weighted','recall_weighted','f1_weighted','roc_auc']\n",
    "scores = cross_validate(clf, X, y, scoring=scoring, cv=10, verbose=2, n_jobs=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average fit_time = 0.4866687774658203\n",
      "Average score_time = 0.024936747550964356\n",
      "Average test_accuracy = 0.788\n",
      "Average test_precision = 0.8630190694658777\n",
      "Average test_recall = 0.8560455192034139\n",
      "Average test_precision_macro = 0.7165816628971723\n",
      "Average test_recall_macro = 0.7180868621658095\n",
      "Average test_f1_macro = 0.7137298208553615\n",
      "Average test_precision_weighted = 0.7902641581796916\n",
      "Average test_recall_weighted = 0.788\n",
      "Average test_f1_weighted = 0.7866815688228124\n",
      "Average test_roc_auc = 0.8624648940438414\n"
     ]
    }
   ],
   "source": [
    "# print(scores)\n",
    "for k,v in scores.items():\n",
    "    print(\"Average \" + k + \" = \" + str(v.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
